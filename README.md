# ğŸ“ Local Voice AI Tutor

A private, locally hosted AI tutor that learns from you. Uses voice, local models, and a personal knowledge graph to offer tailored, evolving guidance. Runs 100% offline in Docker.

---

# Local Voice-Based AI Assistant

This project captures voice from your microphone, transcribes it with Whisper, and sends the text to a local Mistral 7B model for intelligent responsesâ€”all running in Docker locally.

## How It Works
1. User speaks â†’ Microphone records
2. Whisper (faster-whisper) transcribes speech to text
3. Local LLM (Mistral 7B, Ollama, LM Studio, etc.) receives prompt and responds

---

## ğŸš€ Project Overview

**Local Voice AI Tutor** is your private Jarvis.

* ğŸ¤ Talk to it using your voice.
* ğŸš£ï¸ It speaks backâ€”live.
* ğŸ§  It remembers you via a growing **personal knowledge graph**.
* ğŸ“š It answers based on your own **uploaded documents** and past interactions.
* ğŸ”’ Itâ€™s all localâ€”your data never leaves your machine.

Perfect for self-learners, researchers, and anyone who wants a **personal tutor that grows with them**.

---

## ğŸ”§ Features

| Feature            | Description                                                                                                                        |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------------- |
| ğŸ§  Custom LLM      | Run offline using [Ollama](https://ollama.com), [LM Studio](https://lmstudio.ai), or [OpenLLM](https://github.com/bentoml/OpenLLM) |
| ğŸ§¹ RAG Support     | Load PDFs/notes as knowledge base                                                                                                  |
| ğŸ§  Personal Graph  | Neo4j or GraphDB to track your interests and learning                                                                              |
| ğŸ§˜ Voice Interface | Whisper STT + TTS (offline TTS like Piper or Coqui supported)                                                                      |
| ğŸ³ Docker Ready    | One command to deploy locally with privacy-first setup                                                                             |

---

## ğŸ“¦ Project Structure

```
voice-ai-tutor/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py           # FastAPI API server / CLI
â”‚   â”œâ”€â”€ vector_store.py   # Local RAG index (e.g. ChromaDB)
â”‚   â”œâ”€â”€ graph_memory.py   # Knowledge graph logic (Neo4j, RDF, etc.)
â”‚   â”œâ”€â”€ ai_core.py        # Local LLM + prompt logic
â”‚   â”œâ”€â”€ stt.py            # Whisper (offline) transcription
â”‚   â”œâ”€â”€ tts.py            # Offline TTS (Piper/Coqui)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ app.js            # Mic UI and voice loop
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ³ Quickstart (Docker)

```bash
git clone https://github.com/yourname/voice-ai-tutor.git
cd voice-ai-tutor
cp .env.example .env

docker build -t voice-assistant .
docker run --rm -it --device /dev/snd voice-assistant
# Or start everything
# docker-compose up --build
```

> **Note**: Make sure you have Ollama or a compatible LLM running locally in the container (e.g., `llama3`, `mistral`, or `phi`).

---

## âš™ï¸ Custom Knowledge Base

Place your PDFs, Markdown, and notes in the `/knowledge_base` folder. The system will automatically ingest and create a local vector store using Chroma or FAISS.

---

## ğŸ“Š Personal Knowledge Graph

Every conversation updates a user-specific graph that includes:

* Topics learned
* Questions asked
* Strengths/weaknesses
* Suggested next topics

You can visualize this graph using Neo4j Desktop or other tools.

---

## ğŸ§ª Sample `.env.example`

```env
MODEL_NAME=llama3
VECTOR_DB_PATH=./vector_store
KNOWLEDGE_BASE_PATH=./knowledge_base
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
```

---

## ğŸ› ï¸ Requirements

- Python 3.10
- Docker
- Microphone access
- Locally running LLM (Ollama, LM Studio, etc.)
- [Whisper](https://github.com/openai/whisper) (local)
- Optional: Ollama or LM Studio for local LLM

---

## ğŸ“ˆ Roadmap

* [x] Dockerize local LLM + voice loop
* [x] Custom document ingestion with RAG
* [x] Personal knowledge graph via Neo4j
* [ ] Skill assessment + adaptive curriculum
* [ ] Voice emotion detection
* [ ] Plugin architecture (math, code, language learning)

---

## ğŸ” Privacy Note

All user data stays on your machine. No cloud calls. Perfect for learning without surveillance.

---

## ğŸ“œ License

MIT License â€“ build, remix, extend. Just donâ€™t let it forget who taught it ğŸ« 
